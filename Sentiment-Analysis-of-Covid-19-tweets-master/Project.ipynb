{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSBEo5KxhU1l",
        "outputId": "2c0f8338-7c55-4155-8590-668ed913b983"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_m4GW6P7HDR"
      },
      "source": [
        "#importing nltk and installing necessary libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "!pip install emot\n",
        "!pip install twarc\n",
        "!pip install reverse_geocoder\n",
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExEQjCObzjnX"
      },
      "source": [
        "converting csv file to txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6scYzL1C2DPA"
      },
      "source": [
        "import pandas as pd\n",
        "#reading the tweet id's column from csv file\n",
        "dataframe=pd.read_csv(\"/content/drive/MyDrive/sempember_all.csv\", header=None)\n",
        "\n",
        "dataframe=dataframe[0]\n",
        "#converting dataframe having tweet id's into text file\n",
        "dataframe.to_csv(\"/content/drive/MyDrive/september_all.txt\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuzy_APzZlR"
      },
      "source": [
        "extracting Indian specific tweets from global dataset of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXGKHvLG5Xdd"
      },
      "source": [
        "from twarc import Twarc\n",
        "import reverse_geocoder as rg\n",
        "\n",
        "file=open('/content/drive/My Drive/sept_tweet_id_NRT.txt',\"w\")\n",
        "#authenticating credentials\n",
        "consumer_key=\"NcAOmyCah64fNXfiiDoTefngv\"\n",
        "consumer_secret=\"deAqsF7kXvELzvptTAh4JuWgavgYZOfQle6BiTUVqiIlyPdsTq\"\n",
        "access_token=\"1312441901996367872-jB61aTImZI6lJ59jtylGtVbx0mrlhu\"\n",
        "access_token_secret=\"pry9rNZx3vTDr5Rr61Fa55V2AutMXi0l1iH4qhX87ni2F\"\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "count=0\n",
        "#code for hydrating tweets-text from tweet id's\n",
        "for tweet in t.hydrate(open('/content/drive/MyDrive/New Text Document.txt')):\n",
        "  #checking whether it's a Retweet\n",
        "  if tweet['full_text'][0:2] != 'RT':\n",
        "    user=tweet['user']\n",
        "    place=tweet['place']\n",
        "    #checking if the place is there(i.e.. not None) and it is of country \n",
        "    #India then we are storing the tweet in the file as we are considering \n",
        "    #only Indian specific tweets.\n",
        "    if (place is not None) and (place['country_code']=='IN'):\n",
        "      file.write(tweet['id_str']+\"\\n\")\n",
        "      count=count+1\n",
        "      print(count)\n",
        "    #If the place field is None but coordinates is known then we are trying to \n",
        "    #find country-code  using Longitude and Latitude of the tweet's place.\n",
        "    elif tweet['coordinates']:\n",
        "      coordinate=tweet['coordinates']['coordinates']\n",
        "      result=rg.search(coordinate)\n",
        "      if result[0]['cc']=='IN':\n",
        "        file.write(tweet['id_str']+\"\\n\")\n",
        "        count=count+1\n",
        "        print(count)\n",
        "    #if cordinates is also not there then we will try to get tweets from location field\n",
        "    elif 'india' in (user['location']).lower():\n",
        "      file.write(tweet['id_str']+\"\\n\")\n",
        "      count=count+1\n",
        "      print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgj5TrOzpk9"
      },
      "source": [
        "preparing training dataset such that for each tweet we have it's tweet id, tweet text and label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B9cU5f9lsGV"
      },
      "source": [
        "from twarc import Twarc\n",
        "from textblob import TextBlob\n",
        "\n",
        "consumer_key=\"NcAOmyCah64fNXfiiDoTefngv\"\n",
        "consumer_secret=\"deAqsF7kXvELzvptTAh4JuWgavgYZOfQle6BiTUVqiIlyPdsTq\"\n",
        "access_token=\"1312441901996367872-jB61aTImZI6lJ59jtylGtVbx0mrlhu\"\n",
        "access_token_secret=\"pry9rNZx3vTDr5Rr61Fa55V2AutMXi0l1iH4qhX87ni2F\"\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "\n",
        "trainingDataset=[]\n",
        "\n",
        "for tweet in t.hydrate(open('/content/drive/MyDrive/training.txt')):\n",
        "  t={}\n",
        "  t['tweet_id']=tweet['id']\n",
        "  t['text']=tweet['full_text']\n",
        "  polarity=TextBlob(t['text']).sentiment.polarity\n",
        "  if polarity > 0:\n",
        "    t['label']='positive'\n",
        "  elif polarity <0:\n",
        "    t['label']='negative'\n",
        "  else:\n",
        "    t['label']='neutral'\n",
        "  trainingDataset.append(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoNTtZv9z6Qj"
      },
      "source": [
        "preparing evaluation dataset same format as that of training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UML7PHsW1aUp"
      },
      "source": [
        "from twarc import Twarc\n",
        "from textblob import TextBlob\n",
        "consumer_key=\"NcAOmyCah64fNXfiiDoTefngv\"\n",
        "\n",
        "consumer_secret=\"deAqsF7kXvELzvptTAh4JuWgavgYZOfQle6BiTUVqiIlyPdsTq\"\n",
        "\n",
        "access_token=\"1312441901996367872-jB61aTImZI6lJ59jtylGtVbx0mrlhu\"\n",
        "\n",
        "access_token_secret=\"pry9rNZx3vTDr5Rr61Fa55V2AutMXi0l1iH4qhX87ni2F\"\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "\n",
        "evaluationDataset=[]\n",
        "\n",
        "for tweet in t.hydrate(open('/content/drive/MyDrive/evaluation.txt')):\n",
        "  t={}\n",
        "  t['tweet_id']=tweet['id']\n",
        "  t['text']=tweet['full_text']\n",
        "  polarity=TextBlob(t['text']).sentiment.polarity\n",
        "  if polarity > 0:\n",
        "    t['label']='positive'\n",
        "  elif polarity <0:\n",
        "    t['label']='negative'\n",
        "  else:\n",
        "    t['label']='neutral'\n",
        "  evaluationDataset.append(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2g8G-f0CX-"
      },
      "source": [
        "preparing test dataset such that for each tweet we have it's tweet id, tweet text and label. where label is initialized with None value.\n",
        "we have two dataset here one for the month of April and another one for September month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Fr9a3C0nhQ"
      },
      "source": [
        "from twarc import Twarc\n",
        "from textblob import TextBlob\n",
        "consumer_key=\"NcAOmyCah64fNXfiiDoTefngv\"\n",
        "\n",
        "consumer_secret=\"deAqsF7kXvELzvptTAh4JuWgavgYZOfQle6BiTUVqiIlyPdsTq\"\n",
        "\n",
        "access_token=\"1312441901996367872-jB61aTImZI6lJ59jtylGtVbx0mrlhu\"\n",
        "\n",
        "access_token_secret=\"pry9rNZx3vTDr5Rr61Fa55V2AutMXi0l1iH4qhX87ni2F\"\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "testDataset_apr=[]\n",
        "\n",
        "for tweet in t.hydrate(open('/content/drive/MyDrive/test_april.txt')):\n",
        "  t={}\n",
        "  t['tweet_id']=tweet['id']\n",
        "  t['text']=tweet['full_text']\n",
        "  t['label']=None\n",
        "  testDataset_apr.append(t)\n",
        "testDataset_sep=[]\n",
        "for tweet in t.hydrate(open('/content/drive/MyDrive/test_sept.csv')):\n",
        "  t={}\n",
        "  t['tweet_id']=tweet['id']\n",
        "  t['text']=tweet['full_text']\n",
        "  t['label']=None\n",
        "  testDataset_sep.append(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGhs7nfN0fYC"
      },
      "source": [
        "print(len(trainingDataset))\n",
        "print(len(evaluationDataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62HkttQ0X6V"
      },
      "source": [
        "code for preprocessing of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GBSYL7ce9vH"
      },
      "source": [
        "import contractions \n",
        "from nltk.tokenize import word_tokenize\n",
        "import string \n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS# Function for removing emoticons\n",
        "\n",
        "\n",
        "class PreProcessTweets:\n",
        "    _stopwords=[]\n",
        "    def _init_(self):\n",
        "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['ATUSER','URL'])\n",
        "        \n",
        "    def processTweets(self, list_of_tweets):\n",
        "        processedTweets=[]\n",
        "        for tweet in list_of_tweets:\n",
        "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
        "        return processedTweets\n",
        "    \n",
        "    def _processTweet(self, tweet):\n",
        "        tweet = tweet.lower() # convert text to lower-case\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
        "        tweet = re.sub('@[^\\s]+', 'ATUSER', tweet) # remove usernames\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
        "        tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)  #remove the repeating characters\n",
        "        for emot in EMOTICONS:\n",
        "          tweet = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), tweet) \n",
        "        expanded_words = []\t\n",
        "        for word in tweet.split(): \n",
        "          expanded_words.append(contractions.fix(word));\n",
        "        punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "        str1_list =[]\n",
        "        for word1 in expanded_words:\n",
        "          str1 =\"\";\n",
        "          flag =0;\n",
        "          for each_char in word1: \n",
        "            if each_char not in punctuations:\n",
        "              str1 = str1 + each_char\n",
        "              flag =1\n",
        "          if flag == 1:\n",
        "            str1_list.append(str1)\n",
        "        return [word for word in str1_list if word not in self._stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV1UNj7MQWgo"
      },
      "source": [
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNEqIx34TB3K"
      },
      "source": [
        "tweetsProcessor=PreProcessTweets()\n",
        "PPTrainingset=tweetsProcessor.processTweets(trainingDataset)\n",
        "PPEvaluationset=tweetsProcessor.processTweets(evaluationDataset)\n",
        "PPTestset_apr=tweetsProcessor.processTweets(testDataset_apr)\n",
        "PPTestset_sep=tweetsProcessor.processTweets(testDataset_sep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmkAyK_YYhOQ"
      },
      "source": [
        "print(len(PPTrainingset))\n",
        "print(len(PPTestset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFPPRGbu2BIs"
      },
      "source": [
        "creating word feature such that it has all the unique words from our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tan-TQ5aY0Gu"
      },
      "source": [
        "from nltk import FreqDist\n",
        "tokens=[]\n",
        "word_feature=[]\n",
        "for (words,label) in PPTrainingset:\n",
        "  tokens.extend(words)\n",
        "freq_dist_tokens=FreqDist(tokens)\n",
        "word_feature=freq_dist_tokens.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-SnpEP036VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e7a388-487a-416f-a674-241b34a81689"
      },
      "source": [
        "print(len(word_feature))\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48762\n",
            "777686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bojSESXG2MHB"
      },
      "source": [
        "function which takes input as a particular tweet and returns feature vector for  that tweet. feature vector is designed using one hot encoding technique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcJRKgDUa9O-"
      },
      "source": [
        "def extract_features(tweet):\n",
        "  tweet_words = set(tweet)\n",
        "  features = {}\n",
        "  for word in word_feature:\n",
        "    features['contains(%s)' % word] = (word in tweet_words)\n",
        "  return features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dFpGEU12fYb"
      },
      "source": [
        "with the help of apply_features method of NLTK all the feature vectors of all the tweets in training set all stored space efficiently in trainingFeatures object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVopPn2sdAzv"
      },
      "source": [
        "import nltk\n",
        "trainingFeatures = nltk.classify.apply_features(extract_features, PPTrainingset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oePAQCx3SV2"
      },
      "source": [
        "training the NBayes Classifier model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz1NauoZdidn"
      },
      "source": [
        "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsVDPS0jsfWR",
        "outputId": "3929f487-20d4-4911-c4e0-eb42d37165db"
      },
      "source": [
        "#Finding the accuracy of the model with tagged data.\n",
        "count=0\n",
        "for (tweet,label) in PPEvaluationset:\n",
        "  label==NBayesClassifier.classify(extract_features(tweet)):\n",
        "    count+=1\n",
        "Accuracy=(float(count)/len(PPEvaluationset))*100\n",
        "print(Accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71.9978914074855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9tf-adR5Vle"
      },
      "source": [
        "pos_count=0\n",
        "neg_count=0\n",
        "neu_count=0\n",
        "for (tweet,label) in PPtestset_apr:\n",
        "  label=NBayesClassifier.classify(extract_features(tweet))\n",
        "  if label=='positive':\n",
        "    pos_count+=1\n",
        "  elif label=='negative':\n",
        "    neg_count+=1\n",
        "  else:\n",
        "    neu_count+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIdAuEVW6Q40"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "activities = ['positive', 'negative','neutral']\n",
        "  \n",
        "# portion covered by each label\n",
        "slices = [pos_count,neg_count,neu_count]\n",
        "  \n",
        "# color for each label\n",
        "colors = ['g', 'r','b']\n",
        "  \n",
        "# plotting the pie chart\n",
        "plt.pie(slices, labels = activities, colors=colors, \n",
        "        startangle=90, shadow = True, \n",
        "        radius = 1.2, autopct = '%1.1f%%')\n",
        "  \n",
        "# plotting legend\n",
        "plt.legend()\n",
        "  \n",
        "# showing the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy6H_I-5H7aE"
      },
      "source": [
        "pos_count=0\n",
        "neg_count=0\n",
        "neu_count=0\n",
        "for (tweet,label) in PPtestset_sep:\n",
        "  label=NBayesClassifier.classify(extract_features(tweet))\n",
        "  if label=='positive':\n",
        "    pos_count+=1\n",
        "  elif label=='negative':\n",
        "    neg_count+=1\n",
        "  else:\n",
        "    neu_count+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN4ReYQ0LlE0"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "activities = ['positive', 'negative','neutral']\n",
        "  \n",
        "# portion covered by each label\n",
        "slices = [pos_count,neg_count,neu_count]\n",
        "  \n",
        "# color for each label\n",
        "colors = ['g', 'r','b']\n",
        "  \n",
        "# plotting the pie chart\n",
        "plt.pie(slices, labels = activities, colors=colors, \n",
        "        startangle=90, shadow = True, \n",
        "        radius = 1.2, autopct = '%1.1f%%')\n",
        "  \n",
        "# plotting legend\n",
        "plt.legend()\n",
        "  \n",
        "# showing the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}